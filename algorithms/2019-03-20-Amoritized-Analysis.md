
<!-- TOC -->

- [均摊分析](#均摊分析)
  - [均摊分析 VS 传统时间复杂度分析](#均摊分析-vs-传统时间复杂度分析)
  - [为什么要做均摊分析？](#为什么要做均摊分析)
- [计算均摊分析时间](#计算均摊分析时间)
  - [聚集分析](#聚集分析)
  - [记账法](#记账法)
  - [势能法](#势能法)
- [Relative Examples](#relative-examples)
  - [Min-Queue (最小值队列)](#min-queue-最小值队列)

<!-- /TOC -->

# 均摊分析

## 均摊分析 VS 传统时间复杂度分析

传统时间复杂度

* 给定的一个算法，以及同样长度为`N`的不同输入数据（依赖于数据的好坏）－－－比如上图中的右边各种排序算法的结果
  * Best
  * Average
  * Worst
* #(number) of ops is same for the same input()

均摊分析（Amoritized Analysis）

* 对一种数据结构连续的操作
  * 一些操作`Fast`
  * 一些操作`Slow`
* 进行了`N`个操作后的平均花费是多少呢？（这就是均摊分析）

![](/images/2019-03-20-23-39-51.png)

## 为什么要做均摊分析？

均摊分析可以帮助我们了解对一个**数据结构**进行n次操作的真实代价。对于相同的操作，有时候可以非常快，例如在O(1)时间内完成，而有时候则需要O(n)。我们当然可以说这个操作最坏情况下的时间复杂度是O(n)，但是这并不能真实反映它的实际复杂度。通过均摊分析，我们可以证明，**尽管有些操作在最坏情况下是O(n)的时间复杂度，但是均摊下来只需要O(1)。**

理解平摊分析的关键是要理解其是对一个操作序列（操作之间是有相关性的）的开销的分析，而不是对一个程序的开销进行的分析或者对某种操作进行的分析。一个系统总会存在对一个数据结构进行一个序列的操作的情况.

例如：数据扩张、栈操作、二进制1等。还有一个理解关键是：平摊分析的目的是从理论上让人确信一系列操作序列的时间上限（为什么不是不同操作序列简单相加呢？因为操作之间是有关联的，不是完全独立的）。

> 例如，对扩张表，如果低于一半元素进行收缩，那么不是个好想法。可以根据聚集分析法轻松分析出最坏情况下（n/2处不断增加和减少）的每个操作平摊分析代价为O（n）。

# 计算均摊分析时间

> 平摊分析是很有用的，例如，对扩张表，如果低于一半元素进行收缩，那么不是个好想法。可以根据聚集分析法轻松分析出最坏情况下（n/2处不断增加和减少）的每个操作平摊分析代价为O（n）。

计算均摊分析的三种方法：聚集分析，基站分析和势能法

## 聚集分析

聚集分析的思路是，计算n次操作的最坏代价，然后除以n。缺点是对操作类型多样的序列比较难分析。

以数据扩张为例，如果表不满，那么插入操作的开销是O（1）；如果刚好满了，那么先要扩张到原来的两倍，那么这次的插入操作的开销就不是O（1）了。那么该如何判断操作的开销呢？在这种情况下，总结一 中简单粗暴的计算最坏情况作为上界已经不起作用了，简单的说每次操作是O（n）是会让人不敢用的。最好的做法就是平摊分析，将总体的开销平分到每次小的操作上。

![](/images/2019-03-21-00-26-01.png)

图绿色部分的技巧，让每一次的copy平均消费不超过2的事实显露出来。这时候，说增加一个元素的操作的平摊时间为O（1），总体的操作序列（假设长度为k）的平摊分析时间为O（k＊1），即O（k）。
两种描述一个操作时间复杂度的说法：一个是O（log n）是最坏情况，另一个是O（log n）是平摊分析时间。二者说法共同点是，对整体程序的时间复杂度计算来说贡献相同。区别是一个程序是否对一个操作及时响应有要求。

## 记账法

记账法的思路是，对每一个操作进行分析，然后计算总体代价。（总结一最后介绍的思路）

## 势能法

势能法的思路是，认为每一个操作，除了自身的消耗为1以外，还对整体的一个“势”这个概念进行了影响，所以每一步操作的总代价是1加上这一步和上一步的势能差。势函数可以自己定义。例如，在表扩张中，我们可以定义势函数自变量是i，表示第i次操作，表达式为:2*第i次操作后表中的实际元素个数－第i次操作后表的大小(表可以比实际元素大)。这样定义的好处在于，在第i次操作是扩张时候，势函数大小刚好为0，如果不扩张，那么每次都累加势能（例如：当表中有4个元素，但表大小为8，那么继续加入元素，有2＊5-8=2，2＊6-8=4 ...）。只要找到合适的势函数，则可以轻松的用势能法计算出操作序列的上界（即平摊代价）。

# Relative Examples

## Min-Queue (最小值队列)

> 最小值队列，就是要在队列中添加找到最小值的操作。关键是增加、删除、找最小值操作都要在O（1）完成。该如何实现呢？

分析：如果每次都保留最小值，入队时候，那么只需要将新加入的值和以前的最小值进行比较，看是否需要更新最小值。但难点在于出队操作。出队时候，可能将最小值出队，导致无法找到倒数第二小的值了。为什么？如果维持一个从最小值往上替补链接，那么在入队时候，一定要将最小值指向以前的最小值。考虑这样一种情况：第一次入队一个很小的值，后面再来三个值都比第一次值大，最后入一个比第一次入队的值小的值，那么该值的替补指向第一次入队的值。如果不对中间三个值进行大小链维护，那么发生出队操作时替补链条将会崩溃——没有人知道倒数第三大的值是谁。如果对中间三个值进行大小链维护，那么代价至少是O（n），因为对每个新加入的点，都要延着最小值链表进行查找比较，然后插入正确的位置。

解决方法是采用两个背靠背的最小栈，如下图。

![](/images/2019-03-20-23-59-21.png)


对于最小栈结构，如果使用小技巧，那么可以使得栈从底向上会自然的形成最小值替补链条。
具体方法是：采用一个变量保存当前最小值，例如minV。在push的时候，例如push（x），如果push的值x比当前最小值minV小，那么说明：（1）最小值要被更新为x， （2）minV是可以作为将来x被pop出去后的替补值。所以这时所做的操作就是把minV先push到栈中，然后再把x给push到栈中。这样，将来在pop（x）的时候，栈里面实际pop两次，将第二次pop的值恢复成为当前的最小值（做替补）。如果新进的值比最小值大，那么最小值不变，做普通的push操作。原因是不用关心在最小值上方位置存的值（出栈吧，无所谓。因为最小值上方的那么值都比最小值大，他们的出栈不影响最小值）。理解的关键还是在于，更新最小值，那么以前的最小值能刚好作为替补这个关键点。
另外，最小栈可以使用两个栈实现，一个辅助栈保存最小值的替补链条（如果新push的值比该栈顶的值小或等于，则向里面push），pop（x）的时候，如果x与辅助栈的栈顶相等，那么辅助栈也pop一次。
继续来看最小值队列的实现：
蓝栈和黄栈中都维持一个最小值，那么显然队列的最小值是二者中的较小者。从黄栈出，从蓝栈进的一个问题就是，黄栈会枯萎。解决方案是，一旦黄栈枯萎，那么就做一次移动操作，把蓝栈移到右边变成黄栈，耗时O（n）。
现在进行时间复杂度分析：（1）对于查询最小值，时间复杂度是O（1），前面已经解释了，栈维持最小值是很自然的，代价O（1）。
（2）对于出队列操作，普通操作耗费O（1），耗时操作耗O（n）。对于入队操作，一直是O（1）。进行平摊分析，利用记账法，假设每次入队花费两元，剩余一元用于支付一个蓝栈元素移动到黄栈元素。那么出栈消费每次收费O（1）即可，不需要支付移动操作的费用了。所以结果是一次出队（删除）、入队（增加）操作的平摊时间都是O（1）。